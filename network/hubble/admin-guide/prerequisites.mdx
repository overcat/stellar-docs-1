---
title: Prerequisites
sidebar_position: 10
---

Hubble is built with a handful of different components and services to run end-to-end. This includes:

* Access to GCP (Hubble can be run locally or in other cloud services but instructions are only provided for GCP at this time)
* Running Ledger Exporter (TODO: Link to Ledger Exporter docs) to create a LedgerCloseMetaBatch data lake
* Running Airflow to orchestrate running stellar-etl and stellar-dbt-public at a set cadence to insert data into BigQuery
  * Running stellar-etl to ingest from the data lake to create row formatted JSON files for databases to easly upload/insert
  * Running dbt to do useful transformations on the base data from stellar-etl

## GCP Account Setup

The Stellar Development Foundation runs Hubble in GCP. To follow the same deployment you will need to have access to GCP project. Instructions can be found in the [Get Started](https://cloud.google.com/docs/get-started) documentation from Google.

Note: BigQuery and Composer should be available by default. If they are not you can find instructions for enabling them in the [BigQuery](https://cloud.google.com/bigquery?hl=en) or [Composer](https://cloud.google.com/composer?hl=en) Google documentation.

## Ledger Exporter Deployment

TODO: Link to the Ledger Exporter docs

## Create GCP Composer Instance to Run Airflow

Instructions on bringing up a GCP Composer instance to run Hubble can be found in the [Installation and Setup](https://github.com/stellar/stellar-etl-airflow?tab=readme-ov-file#installation-and-setup) section in the [stellar-etl-airflow](https://github.com/stellar/stellar-etl-airflow) repository.

:::note

Hardware requirements can be very different depending on the Stellar network data you require. The default GCP settings may be higher/lower than actually required.

:::